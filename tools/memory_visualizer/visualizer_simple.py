"""
è®°å¿†å›¾å¯è§†åŒ– - ç‹¬ç«‹ç‰ˆæœ¬

ç›´æ¥ä»å­˜å‚¨çš„æ•°æ®æ–‡ä»¶ç”Ÿæˆå¯è§†åŒ–,æ— éœ€å¯åŠ¨å®Œæ•´çš„è®°å¿†ç®¡ç†å™¨
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Set

from pathlib import Path
from typing import Any, Dict, List, Optional, Set

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from flask import Flask, jsonify, render_template_string, request, send_from_directory
from flask_cors import CORS

app = Flask(__name__)
CORS(app)

# æ•°æ®ç¼“å­˜
graph_data_cache = None
data_dir = project_root / "data" / "memory_graph"
current_data_file = None  # å½“å‰é€‰æ‹©çš„æ•°æ®æ–‡ä»¶


def find_available_data_files() -> List[Path]:
    """æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨çš„è®°å¿†å›¾æ•°æ®æ–‡ä»¶"""
    files = []
    
    if not data_dir.exists():
        return files
    
    # æŸ¥æ‰¾å¤šç§å¯èƒ½çš„æ–‡ä»¶å
    possible_files = [
        "graph_store.json",
        "memory_graph.json",
        "graph_data.json",
    ]
    
    for filename in possible_files:
        file_path = data_dir / filename
        if file_path.exists():
            files.append(file_path)
    
    # æŸ¥æ‰¾æ‰€æœ‰å¤‡ä»½æ–‡ä»¶
    for pattern in ["graph_store_*.json", "memory_graph_*.json", "graph_data_*.json"]:
        for backup_file in data_dir.glob(pattern):
            if backup_file not in files:
                files.append(backup_file)
    
    # æŸ¥æ‰¾backupså­ç›®å½•
    backups_dir = data_dir / "backups"
    if backups_dir.exists():
        for backup_file in backups_dir.glob("**/*.json"):
            if backup_file not in files:
                files.append(backup_file)
    
    # æŸ¥æ‰¾data/backupç›®å½•
    backup_dir = data_dir.parent / "backup"
    if backup_dir.exists():
        for backup_file in backup_dir.glob("**/graph_*.json"):
            if backup_file not in files:
                files.append(backup_file)
        for backup_file in backup_dir.glob("**/memory_*.json"):
            if backup_file not in files:
                files.append(backup_file)
    
    return sorted(files, key=lambda f: f.stat().st_mtime, reverse=True)


def load_graph_data(file_path: Optional[Path] = None) -> Dict[str, Any]:
    """ä»ç£ç›˜åŠ è½½å›¾æ•°æ®"""
    global graph_data_cache, current_data_file
    
    # å¦‚æœæŒ‡å®šäº†æ–°æ–‡ä»¶ï¼Œæ¸…é™¤ç¼“å­˜
    if file_path is not None and file_path != current_data_file:
        graph_data_cache = None
        current_data_file = file_path
    
    if graph_data_cache is not None:
        return graph_data_cache
    
    try:
        # ç¡®å®šè¦åŠ è½½çš„æ–‡ä»¶
        if current_data_file is not None:
            graph_file = current_data_file
        else:
            # å°è¯•æŸ¥æ‰¾å¯ç”¨çš„æ•°æ®æ–‡ä»¶
            available_files = find_available_data_files()
            if not available_files:
                print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å›¾æ•°æ®æ–‡ä»¶")
                print(f"ğŸ“‚ æœç´¢ç›®å½•: {data_dir}")
                return {
                    "nodes": [], 
                    "edges": [], 
                    "memories": [],
                    "stats": {"total_nodes": 0, "total_edges": 0, "total_memories": 0},
                    "error": "æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶",
                    "available_files": []
                }
            
            # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
            graph_file = available_files[0]
            current_data_file = graph_file
            print(f"ğŸ“‚ è‡ªåŠ¨é€‰æ‹©æœ€æ–°æ–‡ä»¶: {graph_file}")
        
        if not graph_file.exists():
            print(f"âš ï¸  å›¾æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {graph_file}")
            return {
                "nodes": [], 
                "edges": [], 
                "memories": [],
                "stats": {"total_nodes": 0, "total_edges": 0, "total_memories": 0},
                "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {graph_file}"
            }
        
        print(f"ğŸ“‚ åŠ è½½å›¾æ•°æ®: {graph_file}")
        with open(graph_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # è§£ææ•°æ®
        nodes_dict = {}
        edges_list = []
        memory_info = []
        
        # å®é™…æ–‡ä»¶æ ¼å¼æ˜¯ {nodes: [], edges: [], metadata: {}}
        # ä¸æ˜¯ {memories: [{nodes: [], edges: []}]}
        nodes = data.get("nodes", [])
        edges = data.get("edges", [])
        metadata = data.get("metadata", {})
        
        print(f"âœ… æ‰¾åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹, {len(edges)} æ¡è¾¹")
        
        # å¤„ç†èŠ‚ç‚¹
        for node in nodes:
            node_id = node.get('id', '')
            if node_id and node_id not in nodes_dict:
                memory_ids = node.get('metadata', {}).get('memory_ids', [])
                nodes_dict[node_id] = {
                    'id': node_id,
                    'label': node.get('content', ''),
                    'type': node.get('node_type', ''),
                    'group': extract_group_from_type(node.get('node_type', '')),
                    'title': f"{node.get('node_type', '')}: {node.get('content', '')}",
                    'metadata': node.get('metadata', {}),
                    'created_at': node.get('created_at', ''),
                    'memory_ids': memory_ids,
                }
        
        # å¤„ç†è¾¹ - ä½¿ç”¨é›†åˆå»é‡ï¼Œé¿å…é‡å¤çš„è¾¹ID
        existing_edge_ids = set()
        for edge in edges:
            # è¾¹çš„IDå­—æ®µå¯èƒ½æ˜¯ 'id' æˆ– 'edge_id'
            edge_id = edge.get('edge_id') or edge.get('id', '')
            # å¦‚æœIDä¸ºç©ºæˆ–å·²å­˜åœ¨ï¼Œè·³è¿‡è¿™æ¡è¾¹
            if not edge_id or edge_id in existing_edge_ids:
                continue
            
            existing_edge_ids.add(edge_id)
            memory_id = edge.get('metadata', {}).get('memory_id', '')
            
            # æ³¨æ„: GraphStore ä¿å­˜çš„æ ¼å¼ä½¿ç”¨ 'source'/'target', ä¸æ˜¯ 'source_id'/'target_id'
            edges_list.append({
                'id': edge_id,
                'from': edge.get('source', edge.get('source_id', '')),
                'to': edge.get('target', edge.get('target_id', '')),
                'label': edge.get('relation', ''),
                'type': edge.get('edge_type', ''),
                'importance': edge.get('importance', 0.5),
                'title': f"{edge.get('edge_type', '')}: {edge.get('relation', '')}",
                'arrows': 'to',
                'memory_id': memory_id,
            })
        
        # ä»å…ƒæ•°æ®ä¸­è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = metadata.get('statistics', {})
        total_memories = stats.get('total_memories', 0)
        
        # TODO: å¦‚æœéœ€è¦è®°å¿†è¯¦ç»†ä¿¡æ¯,éœ€è¦ä»å…¶ä»–åœ°æ–¹åŠ è½½
        # ç›®å‰åªæœ‰èŠ‚ç‚¹å’Œè¾¹çš„æ•°æ®
        
        graph_data_cache = {
            'nodes': list(nodes_dict.values()),
            'edges': edges_list,
            'memories': memory_info,  # ç©ºåˆ—è¡¨,å› ä¸ºæ–‡ä»¶ä¸­æ²¡æœ‰è®°å¿†è¯¦æƒ…
            'stats': {
                'total_nodes': len(nodes_dict),
                'total_edges': len(edges_list),
                'total_memories': total_memories,
            },
            'current_file': str(graph_file),
            'file_size': graph_file.stat().st_size,
            'file_modified': datetime.fromtimestamp(graph_file.stat().st_mtime).isoformat(),
        }
        
        print(f"ğŸ“Š ç»Ÿè®¡: {len(nodes_dict)} ä¸ªèŠ‚ç‚¹, {len(edges_list)} æ¡è¾¹, {total_memories} æ¡è®°å¿†")
        print(f"ğŸ“„ æ•°æ®æ–‡ä»¶: {graph_file} ({graph_file.stat().st_size / 1024:.2f} KB)")
        return graph_data_cache
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"nodes": [], "edges": [], "memories": [], "stats": {}}


def extract_group_from_type(node_type: str) -> str:
    """ä»èŠ‚ç‚¹ç±»å‹æå–åˆ†ç»„å"""
    # å‡è®¾ç±»å‹æ ¼å¼ä¸º "ä¸»ä½“" æˆ– "SUBJECT"
    type_mapping = {
        'ä¸»ä½“': 'SUBJECT',
        'ä¸»é¢˜': 'TOPIC',
        'å®¢ä½“': 'OBJECT',
        'å±æ€§': 'ATTRIBUTE',
        'å€¼': 'VALUE',
    }
    return type_mapping.get(node_type, node_type)


def generate_memory_text(memory: Dict[str, Any]) -> str:
    """ç”Ÿæˆè®°å¿†çš„æ–‡æœ¬æè¿°"""
    try:
        nodes = {n['id']: n for n in memory.get('nodes', [])}
        edges = memory.get('edges', [])
        
        subject_id = memory.get('subject_id', '')
        if not subject_id or subject_id not in nodes:
            return f"[è®°å¿† {memory.get('id', '')[:8]}]"
        
        parts = [nodes[subject_id]['content']]
        
        # æ‰¾ä¸»é¢˜èŠ‚ç‚¹
        for edge in edges:
            if edge.get('edge_type') == 'è®°å¿†ç±»å‹' and edge.get('source_id') == subject_id:
                topic_id = edge.get('target_id', '')
                if topic_id in nodes:
                    parts.append(nodes[topic_id]['content'])
                    
                    # æ‰¾å®¢ä½“
                    for e2 in edges:
                        if e2.get('edge_type') == 'æ ¸å¿ƒå…³ç³»' and e2.get('source_id') == topic_id:
                            obj_id = e2.get('target_id', '')
                            if obj_id in nodes:
                                parts.append(f"{e2.get('relation', '')} {nodes[obj_id]['content']}")
                                break
                    break
        
        return " ".join(parts)
    except Exception:
        return f"[è®°å¿† {memory.get('id', '')[:8]}]"


# ä½¿ç”¨å†…åµŒçš„HTMLæ¨¡æ¿(ä¸ä¹‹å‰ç›¸åŒ)
HTML_TEMPLATE = open(project_root / "tools" / "memory_visualizer" / "templates" / "visualizer.html", 'r', encoding='utf-8').read()


@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    return render_template_string(HTML_TEMPLATE)


@app.route('/api/graph/full')
def get_full_graph():
    """è·å–å®Œæ•´è®°å¿†å›¾æ•°æ®"""
    try:
        data = load_graph_data()
        return jsonify({
            'success': True,
            'data': data
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/memory/<memory_id>')
def get_memory_detail(memory_id: str):
    """è·å–è®°å¿†è¯¦æƒ…"""
    try:
        data = load_graph_data()
        memory = next((m for m in data['memories'] if m['id'] == memory_id), None)
        
        if memory is None:
            return jsonify({
                'success': False,
                'error': 'è®°å¿†ä¸å­˜åœ¨'
            }), 404
        
        return jsonify({
            'success': True,
            'data': memory
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/search')
def search_memories():
    """æœç´¢è®°å¿†"""
    try:
        query = request.args.get('q', '').lower()
        limit = int(request.args.get('limit', 50))
        
        data = load_graph_data()
        
        # ç®€å•çš„æ–‡æœ¬åŒ¹é…æœç´¢
        results = []
        for memory in data['memories']:
            text = memory.get('text', '').lower()
            if query in text:
                results.append(memory)
        
        return jsonify({
            'success': True,
            'data': {
                'results': results[:limit],
                'count': len(results),
            }
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/stats')
def get_statistics():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    try:
        data = load_graph_data()
        
        # æ‰©å±•ç»Ÿè®¡ä¿¡æ¯
        node_types = {}
        memory_types = {}
        
        for node in data['nodes']:
            node_type = node.get('type', 'Unknown')
            node_types[node_type] = node_types.get(node_type, 0) + 1
        
        for memory in data['memories']:
            mem_type = memory.get('type', 'Unknown')
            memory_types[mem_type] = memory_types.get(mem_type, 0) + 1
        
        stats = data.get('stats', {})
        stats['node_types'] = node_types
        stats['memory_types'] = memory_types
        
        return jsonify({
            'success': True,
            'data': stats
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/reload')
def reload_data():
    """é‡æ–°åŠ è½½æ•°æ®"""
    global graph_data_cache
    graph_data_cache = None
    data = load_graph_data()
    return jsonify({
        'success': True,
        'message': 'æ•°æ®å·²é‡æ–°åŠ è½½',
        'stats': data.get('stats', {})
    })


@app.route('/api/files')
def list_files():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®æ–‡ä»¶"""
    try:
        files = find_available_data_files()
        file_list = []
        
        for f in files:
            stat = f.stat()
            file_list.append({
                'path': str(f),
                'name': f.name,
                'size': stat.st_size,
                'size_kb': round(stat.st_size / 1024, 2),
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'modified_readable': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                'is_current': str(f) == str(current_data_file) if current_data_file else False
            })
        
        return jsonify({
            'success': True,
            'files': file_list,
            'count': len(file_list),
            'current_file': str(current_data_file) if current_data_file else None
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/select_file', methods=['POST'])
def select_file():
    """é€‰æ‹©è¦åŠ è½½çš„æ•°æ®æ–‡ä»¶"""
    global graph_data_cache, current_data_file
    
    try:
        data = request.get_json()
        file_path = data.get('file_path')
        
        if not file_path:
            return jsonify({
                'success': False,
                'error': 'æœªæä¾›æ–‡ä»¶è·¯å¾„'
            }), 400
        
        file_path = Path(file_path)
        if not file_path.exists():
            return jsonify({
                'success': False,
                'error': f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}'
            }), 404
        
        # æ¸…é™¤ç¼“å­˜å¹¶åŠ è½½æ–°æ–‡ä»¶
        graph_data_cache = None
        current_data_file = file_path
        graph_data = load_graph_data(file_path)
        
        return jsonify({
            'success': True,
            'message': f'å·²åˆ‡æ¢åˆ°æ–‡ä»¶: {file_path.name}',
            'stats': graph_data.get('stats', {})
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


def run_server(host: str = '127.0.0.1', port: int = 5001, debug: bool = False):
    """å¯åŠ¨æœåŠ¡å™¨"""
    print("=" * 60)
    print("ğŸ¦Š MoFox Bot - è®°å¿†å›¾å¯è§†åŒ–å·¥å…· (ç‹¬ç«‹ç‰ˆ)")
    print("=" * 60)
    print(f"ğŸ“‚ æ•°æ®ç›®å½•: {data_dir}")
    print(f"ğŸŒ è®¿é—®åœ°å€: http://{host}:{port}")
    print("â¹ï¸  æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    print("=" * 60)
    print()
    
    # é¢„åŠ è½½æ•°æ®
    load_graph_data()
    
    app.run(host=host, port=port, debug=debug)


if __name__ == '__main__':
    try:
        run_server(debug=True)
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢")
    except Exception as e:
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)
