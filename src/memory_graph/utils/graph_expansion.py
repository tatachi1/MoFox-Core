"""
å›¾æ‰©å±•å·¥å…·ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

æä¾›è®°å¿†å›¾çš„æ‰©å±•ç®—æ³•ï¼Œç”¨äºä»åˆå§‹è®°å¿†é›†åˆæ²¿å›¾ç»“æ„æ‰©å±•æŸ¥æ‰¾ç›¸å…³è®°å¿†ã€‚
ä¼˜åŒ–é‡ç‚¹ï¼š
1. æ”¹è¿›BFSéå†æ•ˆç‡
2. æ‰¹é‡å‘é‡æ£€ç´¢ï¼Œå‡å°‘æ•°æ®åº“è°ƒç”¨
3. æ—©åœæœºåˆ¶ï¼Œé¿å…ä¸å¿…è¦çš„æ‰©å±•
4. æ›´æ¸…æ™°çš„æ—¥å¿—è¾“å‡º
"""

import asyncio
from typing import TYPE_CHECKING

from src.common.logger import get_logger
from src.memory_graph.utils.similarity import cosine_similarity

if TYPE_CHECKING:
    import numpy as np

    from src.memory_graph.storage.graph_store import GraphStore
    from src.memory_graph.storage.vector_store import VectorStore

logger = get_logger(__name__)


async def expand_memories_with_semantic_filter(
    graph_store: "GraphStore",
    vector_store: "VectorStore",
    initial_memory_ids: list[str],
    query_embedding: "np.ndarray",
    max_depth: int = 2,
    semantic_threshold: float = 0.5,
    max_expanded: int = 20,
) -> list[tuple[str, float]]:
    """
    ä»åˆå§‹è®°å¿†é›†åˆå‡ºå‘ï¼Œæ²¿å›¾ç»“æ„æ‰©å±•ï¼Œå¹¶ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    è¿™ä¸ªæ–¹æ³•è§£å†³äº†çº¯å‘é‡æœç´¢å¯èƒ½é—æ¼çš„"è¯­ä¹‰ç›¸å…³ä¸”å›¾ç»“æ„ç›¸å…³"çš„è®°å¿†ã€‚

    ä¼˜åŒ–æ”¹è¿›ï¼š
    - ä½¿ç”¨è®°å¿†çº§åˆ«çš„BFSï¼Œè€ŒéèŠ‚ç‚¹çº§åˆ«ï¼ˆæ›´ç›´æ¥ï¼‰
    - æ‰¹é‡è·å–é‚»å±…è®°å¿†ï¼Œå‡å°‘éå†æ¬¡æ•°
    - æ—©åœæœºåˆ¶ï¼šè¾¾åˆ°max_expandedåç«‹å³åœæ­¢
    - æ›´è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—

    Args:
        graph_store: å›¾å­˜å‚¨
        vector_store: å‘é‡å­˜å‚¨
        initial_memory_ids: åˆå§‹è®°å¿†IDé›†åˆï¼ˆç”±å‘é‡æœç´¢å¾—åˆ°ï¼‰
        query_embedding: æŸ¥è¯¢å‘é‡
        max_depth: æœ€å¤§æ‰©å±•æ·±åº¦ï¼ˆ1-3æ¨èï¼‰
        semantic_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.5æ¨èï¼‰
        max_expanded: æœ€å¤šæ‰©å±•å¤šå°‘ä¸ªè®°å¿†

    Returns:
        List[(memory_id, relevance_score)] æŒ‰ç›¸å…³åº¦æ’åº
    """
    if not initial_memory_ids or query_embedding is None:
        return []

    try:
        import time
        start_time = time.time()
        
        # è®°å½•å·²è®¿é—®çš„è®°å¿†ï¼Œé¿å…é‡å¤
        visited_memories = set(initial_memory_ids)
        # è®°å½•æ‰©å±•çš„è®°å¿†åŠå…¶åˆ†æ•°
        expanded_memories: dict[str, float] = {}

        # BFSæ‰©å±•ï¼ˆåŸºäºè®°å¿†è€ŒéèŠ‚ç‚¹ï¼‰
        current_level_memories = initial_memory_ids
        depth_stats = []  # æ¯å±‚ç»Ÿè®¡

        for depth in range(max_depth):
            next_level_memories = []
            candidates_checked = 0
            candidates_passed = 0

            logger.debug(f"ğŸ” å›¾æ‰©å±• - æ·±åº¦ {depth+1}/{max_depth}, å½“å‰å±‚è®°å¿†æ•°: {len(current_level_memories)}")

            # éå†å½“å‰å±‚çš„è®°å¿†
            for memory_id in current_level_memories:
                memory = graph_store.get_memory_by_id(memory_id)
                if not memory:
                    continue

                # è·å–è¯¥è®°å¿†çš„é‚»å±…è®°å¿†ï¼ˆé€šè¿‡è¾¹å…³ç³»ï¼‰
                neighbor_memory_ids = set()
                
                # éå†è®°å¿†çš„æ‰€æœ‰è¾¹ï¼Œæ”¶é›†é‚»å±…è®°å¿†
                for edge in memory.edges:
                    # è·å–è¾¹çš„ç›®æ ‡èŠ‚ç‚¹
                    target_node_id = edge.target_id
                    source_node_id = edge.source_id
                    
                    # é€šè¿‡èŠ‚ç‚¹æ‰¾åˆ°å…¶ä»–è®°å¿†
                    for node_id in [target_node_id, source_node_id]:
                        if node_id in graph_store.node_to_memories:
                            neighbor_memory_ids.update(graph_store.node_to_memories[node_id])
                
                # è¿‡æ»¤æ‰å·²è®¿é—®çš„å’Œè‡ªå·±
                neighbor_memory_ids.discard(memory_id)
                neighbor_memory_ids -= visited_memories

                # æ‰¹é‡è¯„ä¼°é‚»å±…è®°å¿†
                for neighbor_mem_id in neighbor_memory_ids:
                    candidates_checked += 1
                    
                    neighbor_memory = graph_store.get_memory_by_id(neighbor_mem_id)
                    if not neighbor_memory:
                        continue

                    # è·å–é‚»å±…è®°å¿†çš„ä¸»é¢˜èŠ‚ç‚¹å‘é‡
                    topic_node = next(
                        (n for n in neighbor_memory.nodes if n.has_embedding()),
                        None
                    )
                    
                    if not topic_node or topic_node.embedding is None:
                        continue

                    # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
                    semantic_sim = cosine_similarity(query_embedding, topic_node.embedding)

                    # è®¡ç®—è¾¹çš„é‡è¦æ€§ï¼ˆå½±å“è¯„åˆ†ï¼‰
                    edge_importance = neighbor_memory.importance * 0.5  # ä½¿ç”¨è®°å¿†é‡è¦æ€§ä½œä¸ºè¾¹æƒé‡

                    # ç»¼åˆè¯„åˆ†ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦(70%) + é‡è¦æ€§(20%) + æ·±åº¦è¡°å‡(10%)
                    depth_decay = 1.0 / (depth + 2)  # æ·±åº¦è¡°å‡
                    relevance_score = semantic_sim * 0.7 + edge_importance * 0.2 + depth_decay * 0.1

                    # åªä¿ç•™è¶…è¿‡é˜ˆå€¼çš„
                    if relevance_score < semantic_threshold:
                        continue

                    candidates_passed += 1

                    # è®°å½•æ‰©å±•çš„è®°å¿†
                    if neighbor_mem_id not in expanded_memories:
                        expanded_memories[neighbor_mem_id] = relevance_score
                        visited_memories.add(neighbor_mem_id)
                        next_level_memories.append(neighbor_mem_id)
                    else:
                        # å¦‚æœå·²å­˜åœ¨ï¼Œå–æœ€é«˜åˆ†
                        expanded_memories[neighbor_mem_id] = max(
                            expanded_memories[neighbor_mem_id], relevance_score
                        )

                    # æ—©åœï¼šè¾¾åˆ°æœ€å¤§æ‰©å±•æ•°é‡
                    if len(expanded_memories) >= max_expanded:
                        logger.debug(f"â¹ï¸  æå‰åœæ­¢ï¼šå·²è¾¾åˆ°æœ€å¤§æ‰©å±•æ•°é‡ {max_expanded}")
                        break
                
                # æ—©åœæ£€æŸ¥
                if len(expanded_memories) >= max_expanded:
                    break
            
            # è®°å½•æœ¬å±‚ç»Ÿè®¡
            depth_stats.append({
                "depth": depth + 1,
                "checked": candidates_checked,
                "passed": candidates_passed,
                "expanded_total": len(expanded_memories)
            })

            # å¦‚æœæ²¡æœ‰æ–°è®°å¿†æˆ–å·²è¾¾åˆ°æ•°é‡é™åˆ¶ï¼Œæå‰ç»ˆæ­¢
            if not next_level_memories or len(expanded_memories) >= max_expanded:
                logger.debug(f"â¹ï¸  åœæ­¢æ‰©å±•ï¼š{'æ— æ–°è®°å¿†' if not next_level_memories else 'è¾¾åˆ°ä¸Šé™'}")
                break

            # é™åˆ¶ä¸‹ä¸€å±‚çš„è®°å¿†æ•°é‡ï¼Œé¿å…çˆ†ç‚¸æ€§å¢é•¿
            current_level_memories = next_level_memories[:max_expanded]
            
            # æ¯å±‚è®©å‡ºæ§åˆ¶æƒ
            await asyncio.sleep(0.001)

        # æ’åºå¹¶è¿”å›
        sorted_results = sorted(expanded_memories.items(), key=lambda x: x[1], reverse=True)[:max_expanded]
        
        elapsed = time.time() - start_time
        logger.info(
            f"âœ… å›¾æ‰©å±•å®Œæˆ: åˆå§‹{len(initial_memory_ids)}ä¸ª â†’ "
            f"æ‰©å±•{len(sorted_results)}ä¸ªæ–°è®°å¿† "
            f"(æ·±åº¦={max_depth}, é˜ˆå€¼={semantic_threshold:.2f}, è€—æ—¶={elapsed:.3f}s)"
        )
        
        # è¾“å‡ºæ¯å±‚ç»Ÿè®¡
        for stat in depth_stats:
            logger.debug(
                f"  æ·±åº¦{stat['depth']}: æ£€æŸ¥{stat['checked']}ä¸ª, "
                f"é€šè¿‡{stat['passed']}ä¸ª, ç´¯è®¡æ‰©å±•{stat['expanded_total']}ä¸ª"
            )

        return sorted_results

    except Exception as e:
        logger.error(f"è¯­ä¹‰å›¾æ‰©å±•å¤±è´¥: {e}", exc_info=True)
        return []


__all__ = ["expand_memories_with_semantic_filter"]
