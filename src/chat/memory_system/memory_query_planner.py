# -*- coding: utf-8 -*-
"""è®°å¿†æ£€ç´¢æŸ¥è¯¢è§„åˆ’å™¨"""

from __future__ import annotations

import re
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

import orjson

from src.chat.memory_system.memory_chunk import MemoryType
from src.common.logger import get_logger
from src.llm_models.utils_model import LLMRequest

logger = get_logger(__name__)


@dataclass
class MemoryQueryPlan:
    """æŸ¥è¯¢è§„åˆ’ç»“æžœ"""

    semantic_query: str
    memory_types: List[MemoryType] = field(default_factory=list)
    subject_includes: List[str] = field(default_factory=list)
    object_includes: List[str] = field(default_factory=list)
    required_keywords: List[str] = field(default_factory=list)
    optional_keywords: List[str] = field(default_factory=list)
    owner_filters: List[str] = field(default_factory=list)
    recency_preference: str = "any"
    limit: int = 10
    emphasis: Optional[str] = None
    raw_plan: Dict[str, Any] = field(default_factory=dict)

    def ensure_defaults(self, fallback_query: str, default_limit: int) -> None:
        if not self.semantic_query:
            self.semantic_query = fallback_query
        if self.limit <= 0:
            self.limit = default_limit
        self.recency_preference = (self.recency_preference or "any").lower()
        if self.recency_preference not in {"any", "recent", "historical"}:
            self.recency_preference = "any"
        self.emphasis = (self.emphasis or "balanced").lower()


class MemoryQueryPlanner:
    """åŸºäºŽå°æ¨¡åž‹çš„è®°å¿†æ£€ç´¢æŸ¥è¯¢è§„åˆ’å™¨"""

    def __init__(self, planner_model: Optional[LLMRequest], default_limit: int = 10):
        self.model = planner_model
        self.default_limit = default_limit

    async def plan_query(self, query_text: str, context: Dict[str, Any]) -> MemoryQueryPlan:
        if not self.model:
            logger.debug("æœªæä¾›æŸ¥è¯¢è§„åˆ’æ¨¡åž‹ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ’")
            return self._default_plan(query_text)

        prompt = self._build_prompt(query_text, context)

        try:
            response, _ = await self.model.generate_response_async(prompt, temperature=0.2)
            payload = self._extract_json_payload(response)
            if not payload:
                logger.debug("æŸ¥è¯¢è§„åˆ’æ¨¡åž‹æœªè¿”å›žç»“æž„åŒ–ç»“æžœï¼Œä½¿ç”¨é»˜è®¤è§„åˆ’")
                return self._default_plan(query_text)

            try:
                data = orjson.loads(payload)
            except orjson.JSONDecodeError as exc:
                preview = payload[:200]
                logger.warning("è§£æžæŸ¥è¯¢è§„åˆ’JSONå¤±è´¥: %sï¼Œç‰‡æ®µ: %s", exc, preview)
                return self._default_plan(query_text)

            plan = self._parse_plan_dict(data, query_text)
            plan.ensure_defaults(query_text, self.default_limit)
            return plan

        except Exception as exc:
            logger.error("æŸ¥è¯¢è§„åˆ’æ¨¡åž‹è°ƒç”¨å¤±è´¥: %s", exc, exc_info=True)
            return self._default_plan(query_text)

    def _default_plan(self, query_text: str) -> MemoryQueryPlan:
        return MemoryQueryPlan(
            semantic_query=query_text,
            limit=self.default_limit
        )

    def _parse_plan_dict(self, data: Dict[str, Any], fallback_query: str) -> MemoryQueryPlan:
        semantic_query = self._safe_str(data.get("semantic_query")) or fallback_query

        def _collect_list(key: str) -> List[str]:
            value = data.get(key)
            if isinstance(value, str):
                return [value]
            if isinstance(value, list):
                return [self._safe_str(item) for item in value if self._safe_str(item)]
            return []

        memory_type_values = _collect_list("memory_types")
        memory_types: List[MemoryType] = []
        for item in memory_type_values:
            if not item:
                continue
            try:
                memory_types.append(MemoryType(item))
            except ValueError:
                # å°è¯•åŒ¹é…valueå€¼
                normalized = item.lower()
                for mt in MemoryType:
                    if mt.value == normalized:
                        memory_types.append(mt)
                        break

        plan = MemoryQueryPlan(
            semantic_query=semantic_query,
            memory_types=memory_types,
            subject_includes=_collect_list("subject_includes"),
            object_includes=_collect_list("object_includes"),
            required_keywords=_collect_list("required_keywords"),
            optional_keywords=_collect_list("optional_keywords"),
            owner_filters=_collect_list("owner_filters"),
            recency_preference=self._safe_str(data.get("recency")) or "any",
            limit=self._safe_int(data.get("limit"), self.default_limit),
            emphasis=self._safe_str(data.get("emphasis")) or "balanced",
            raw_plan=data
        )
        return plan

    def _build_prompt(self, query_text: str, context: Dict[str, Any]) -> str:
        participants = context.get("participants") or context.get("speaker_names") or []
        if isinstance(participants, str):
            participants = [participants]
        participants = [p for p in participants if isinstance(p, str) and p.strip()]
        participant_preview = "ã€".join(participants[:5]) or "æœªçŸ¥"

        persona = context.get("bot_personality") or context.get("bot_identity") or "æœªçŸ¥"

        # æž„å»ºæœªè¯»æ¶ˆæ¯ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_section = ""
        if context.get("has_unread_context") and context.get("unread_messages_context"):
            unread_context = context["unread_messages_context"]
            unread_messages = unread_context.get("messages", [])
            unread_keywords = unread_context.get("keywords", [])
            unread_participants = unread_context.get("participants", [])
            context_summary = unread_context.get("context_summary", "")

            if unread_messages:
                # æž„å»ºæœªè¯»æ¶ˆæ¯æ‘˜è¦
                message_previews = []
                for msg in unread_messages[:5]:  # æœ€å¤šæ˜¾ç¤º5æ¡
                    sender = msg.get("sender", "æœªçŸ¥")
                    content = msg.get("content", "")[:100]  # é™åˆ¶æ¯æ¡æ¶ˆæ¯é•¿åº¦
                    message_previews.append(f"{sender}: {content}")

                context_section = f"""

## ðŸ“‹ æœªè¯»æ¶ˆæ¯ä¸Šä¸‹æ–‡ (å…±{unread_context.get('total_count', 0)}æ¡æœªè¯»æ¶ˆæ¯)
### æœ€è¿‘æ¶ˆæ¯é¢„è§ˆ:
{chr(10).join(message_previews)}

### ä¸Šä¸‹æ–‡å…³é”®è¯:
{', '.join(unread_keywords[:15]) if unread_keywords else 'æ— '}

### å¯¹è¯å‚ä¸Žè€…:
{', '.join(unread_participants) if unread_participants else 'æ— '}

### ä¸Šä¸‹æ–‡æ‘˜è¦:
{context_summary[:300] if context_summary else 'æ— '}
"""
        else:
            context_section = """

## ðŸ“‹ æœªè¯»æ¶ˆæ¯ä¸Šä¸‹æ–‡:
æ— æœªè¯»æ¶ˆæ¯æˆ–ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸å¯ç”¨
"""

        return f"""
ä½ æ˜¯ä¸€åè®°å¿†æ£€ç´¢è§„åˆ’åŠ©æ‰‹ï¼Œè¯·åŸºäºŽè¾“å…¥ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ JSON æ£€ç´¢è®¡åˆ’ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ†æžå½“å‰æŸ¥è¯¢å¹¶ç»“åˆæœªè¯»æ¶ˆæ¯çš„ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆæ›´ç²¾å‡†çš„è®°å¿†æ£€ç´¢ç­–ç•¥ã€‚

ä»…éœ€æä¾›ä»¥ä¸‹å­—æ®µï¼š
- semantic_query: ç”¨äºŽå‘é‡å¬å›žçš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œè¦æ±‚å…·ä½“ä¸”è´´åˆå½“å‰æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ï¼›
- memory_types: å»ºè®®æ£€ç´¢çš„è®°å¿†ç±»åž‹åˆ—è¡¨ï¼Œå–å€¼èŒƒå›´æ¥è‡ª MemoryType æžšä¸¾ (personal_fact,event,preference,opinion,relationship,emotion,knowledge,skill,goal,experience,contextual)ï¼›
- subject_includes: å»ºè®®å‡ºçŽ°åœ¨è®°å¿†ä¸»è¯­ä¸­çš„äººç‰©æˆ–è§’è‰²ï¼›
- object_includes: å»ºè®®å…³æ³¨çš„å¯¹è±¡ã€ä¸»é¢˜æˆ–å…³é”®ä¿¡æ¯ï¼›
- required_keywords: å»ºè®®å¿…é¡»åŒ…å«çš„å…³é”®è¯ï¼ˆä»Žä¸Šä¸‹æ–‡ä¸­æå–ï¼‰ï¼›
- recency: æŽ¨èçš„æ—¶é—´åå¥½ï¼Œå¯é€‰ recent/any/historicalï¼›
- limit: æŽ¨èçš„æœ€å¤§è¿”å›žæ•°é‡ (1-15)ï¼›
- emphasis: æ£€ç´¢é‡ç‚¹ï¼Œå¯é€‰ balanced/contextual/recent/comprehensiveã€‚

è¯·ä¸è¦ç”Ÿæˆè°“è¯­å­—æ®µï¼Œä¹Ÿä¸è¦é¢å¤–è¡¥å……å…¶å®ƒå‚æ•°ã€‚

## å½“å‰æŸ¥è¯¢:
"{query_text}"

## å·²çŸ¥å¯¹è¯å‚ä¸Žè€…:
{participant_preview}

## æœºå™¨äººè®¾å®š:
{persona}{context_section}

## ðŸŽ¯ æŒ‡å¯¼åŽŸåˆ™:
1. **ä¸Šä¸‹æ–‡å…³è”**: ä¼˜å…ˆåˆ†æžä¸Žå½“å‰æŸ¥è¯¢ç›¸å…³çš„æœªè¯»æ¶ˆæ¯å†…å®¹å’Œå…³é”®è¯
2. **è¯­ä¹‰ç†è§£**: ç»“åˆä¸Šä¸‹æ–‡ç†è§£æŸ¥è¯¢çš„çœŸå®žæ„å›¾ï¼Œè€Œéžå­—é¢æ„æ€
3. **å‚ä¸Žè€…æ„ŸçŸ¥**: è€ƒè™‘æœªè¯»æ¶ˆæ¯ä¸­çš„å‚ä¸Žè€…ï¼Œæ£€ç´¢ä¸Žä»–ä»¬ç›¸å…³çš„è®°å¿†
4. **ä¸»é¢˜å»¶ç»­**: å…³æ³¨æœªè¯»æ¶ˆæ¯ä¸­è®¨è®ºçš„ä¸»é¢˜ï¼Œæ£€ç´¢ç›¸å…³çš„åŽ†å²è®°å¿†
5. **æ—¶é—´ç›¸å…³æ€§**: å¦‚æžœæœªè¯»æ¶ˆæ¯è®¨è®ºæœ€è¿‘çš„äº‹ä»¶ï¼Œåå‘æ£€ç´¢ç›¸å…³æ—¶æœŸçš„è®°å¿†

è¯·ç›´æŽ¥è¾“å‡ºç¬¦åˆè¦æ±‚çš„ JSON å¯¹è±¡ï¼Œç¦æ­¢æ·»åŠ é¢å¤–æ–‡æœ¬æˆ– Markdown ä»£ç å—ã€‚
"""

    def _extract_json_payload(self, response: str) -> Optional[str]:
        if not response:
            return None

        stripped = response.strip()
        code_block_match = re.search(r"```(?:json)?\s*(.*?)```", stripped, re.IGNORECASE | re.DOTALL)
        if code_block_match:
            candidate = code_block_match.group(1).strip()
            if candidate:
                return candidate

        start = stripped.find("{")
        end = stripped.rfind("}")
        if start != -1 and end != -1 and end > start:
            return stripped[start:end + 1]

        return stripped if stripped.startswith("{") and stripped.endswith("}") else None

    @staticmethod
    def _safe_str(value: Any) -> str:
        if isinstance(value, str):
            return value.strip()
        if value is None:
            return ""
        return str(value).strip()

    @staticmethod
    def _safe_int(value: Any, default: int) -> int:
        try:
            number = int(value)
            if number <= 0:
                return default
            return number
        except (TypeError, ValueError):
            return default