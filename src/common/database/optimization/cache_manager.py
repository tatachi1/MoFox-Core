"""å¤šçº§ç¼“å­˜ç®¡ç†å™¨

å®ç°é«˜æ€§èƒ½çš„å¤šçº§ç¼“å­˜ç³»ç»Ÿï¼š
- L1ç¼“å­˜ï¼šå†…å­˜ç¼“å­˜ï¼Œ1000é¡¹ï¼Œ60ç§’TTLï¼Œç”¨äºçƒ­ç‚¹æ•°æ®
- L2ç¼“å­˜ï¼šæ‰©å±•ç¼“å­˜ï¼Œ10000é¡¹ï¼Œ300ç§’TTLï¼Œç”¨äºæ¸©æ•°æ®
- LRUæ·˜æ±°ç­–ç•¥ï¼šè‡ªåŠ¨æ·˜æ±°æœ€å°‘ä½¿ç”¨çš„æ•°æ®
- æ™ºèƒ½é¢„çƒ­ï¼šå¯åŠ¨æ—¶é¢„åŠ è½½é«˜é¢‘æ•°æ®
- ç»Ÿè®¡ä¿¡æ¯ï¼šå‘½ä¸­ç‡ã€æ·˜æ±°ç‡ç­‰ç›‘æ§æ•°æ®
"""

import asyncio
import time
from collections import OrderedDict
from collections.abc import Callable
from dataclasses import dataclass
from typing import Any, Generic, TypeVar

from src.common.logger import get_logger
from src.common.memory_utils import estimate_size_smart

logger = get_logger("cache_manager")

T = TypeVar("T")


@dataclass
class CacheEntry(Generic[T]):
    """ç¼“å­˜æ¡ç›®

    Attributes:
        value: ç¼“å­˜çš„å€¼
        created_at: åˆ›å»ºæ—¶é—´æˆ³
        last_accessed: æœ€åè®¿é—®æ—¶é—´æˆ³
        access_count: è®¿é—®æ¬¡æ•°
        size: æ•°æ®å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    """
    value: T
    created_at: float
    last_accessed: float
    access_count: int = 0
    size: int = 0


@dataclass
class CacheStats:
    """ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

    Attributes:
        hits: å‘½ä¸­æ¬¡æ•°
        misses: æœªå‘½ä¸­æ¬¡æ•°
        evictions: æ·˜æ±°æ¬¡æ•°
        total_size: æ€»å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        item_count: æ¡ç›®æ•°é‡
    """
    hits: int = 0
    misses: int = 0
    evictions: int = 0
    total_size: int = 0
    item_count: int = 0

    @property
    def hit_rate(self) -> float:
        """å‘½ä¸­ç‡"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

    @property
    def eviction_rate(self) -> float:
        """æ·˜æ±°ç‡"""
        return self.evictions / self.item_count if self.item_count > 0 else 0.0


class LRUCache(Generic[T]):
    """LRUç¼“å­˜å®ç°

    ä½¿ç”¨OrderedDictå®ç°O(1)çš„get/setæ“ä½œ
    """

    def __init__(
        self,
        max_size: int,
        ttl: float,
        name: str = "cache",
    ):
        """åˆå§‹åŒ–LRUç¼“å­˜

        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            name: ç¼“å­˜åç§°ï¼Œç”¨äºæ—¥å¿—
        """
        self.max_size = max_size
        self.ttl = ttl
        self.name = name
        self._cache: OrderedDict[str, CacheEntry[T]] = OrderedDict()
        self._lock = asyncio.Lock()
        self._stats = CacheStats()

    async def get(self, key: str) -> T | None:
        """è·å–ç¼“å­˜å€¼

        Args:
            key: ç¼“å­˜é”®

        Returns:
            ç¼“å­˜å€¼ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–å·²è¿‡æœŸè¿”å›None
        """
        async with self._lock:
            entry = self._cache.get(key)

            if entry is None:
                self._stats.misses += 1
                return None

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            now = time.time()
            if now - entry.created_at > self.ttl:
                # è¿‡æœŸï¼Œåˆ é™¤æ¡ç›®
                del self._cache[key]
                self._stats.misses += 1
                self._stats.evictions += 1
                self._stats.item_count -= 1
                self._stats.total_size -= entry.size
                return None

            # å‘½ä¸­ï¼Œæ›´æ–°è®¿é—®ä¿¡æ¯
            entry.last_accessed = now
            entry.access_count += 1
            self._stats.hits += 1

            # ç§»åˆ°æœ«å°¾ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
            self._cache.move_to_end(key)

            return entry.value

    async def set(
        self,
        key: str,
        value: T,
        size: int | None = None,
        ttl: float | None = None,
    ) -> None:
        """è®¾ç½®ç¼“å­˜å€¼

        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
            size: æ•°æ®å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™å°è¯•ä¼°ç®—
            ttl: è‡ªå®šä¹‰è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤TTL
        """
        async with self._lock:
            now = time.time()

            # å¦‚æœé”®å·²å­˜åœ¨ï¼Œæ›´æ–°å€¼
            if key in self._cache:
                old_entry = self._cache[key]
                self._stats.total_size -= old_entry.size

            # ä¼°ç®—å¤§å°
            if size is None:
                size = self._estimate_size(value)

            # åˆ›å»ºæ–°æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº†ttlï¼Œåˆ™ä¿®æ”¹created_atæ¥å®ç°è‡ªå®šä¹‰TTLï¼‰
            # é€šè¿‡è°ƒæ•´created_atï¼Œä½¿å¾—: now - created_at + custom_ttl = self.ttl
            # å³: created_at = now - (self.ttl - custom_ttl)
            if ttl is not None and ttl != self.ttl:
                # è°ƒæ•´åˆ›å»ºæ—¶é—´ä»¥å®ç°è‡ªå®šä¹‰TTL
                adjusted_created_at = now - (self.ttl - ttl)
                logger.debug(
                    f"[{self.name}] ä½¿ç”¨è‡ªå®šä¹‰TTL {ttl}s (é»˜è®¤{self.ttl}s) for key: {key}"
                )
            else:
                adjusted_created_at = now

            entry = CacheEntry(
                value=value,
                created_at=adjusted_created_at,
                last_accessed=now,
                access_count=0,
                size=size,
            )

            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œæ·˜æ±°æœ€ä¹…æœªä½¿ç”¨çš„æ¡ç›®
            while len(self._cache) >= self.max_size:
                oldest_key, oldest_entry = self._cache.popitem(last=False)
                self._stats.evictions += 1
                self._stats.item_count -= 1
                self._stats.total_size -= oldest_entry.size
                logger.debug(
                    f"[{self.name}] æ·˜æ±°ç¼“å­˜æ¡ç›®: {oldest_key} "
                    f"(è®¿é—®{oldest_entry.access_count}æ¬¡)"
                )

            # æ·»åŠ æ–°æ¡ç›®
            self._cache[key] = entry
            self._stats.item_count += 1
            self._stats.total_size += size

    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜æ¡ç›®

        Args:
            key: ç¼“å­˜é”®

        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        async with self._lock:
            entry = self._cache.pop(key, None)
            if entry:
                self._stats.item_count -= 1
                self._stats.total_size -= entry.size
                return True
            return False

    async def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        async with self._lock:
            self._cache.clear()
            self._stats = CacheStats()

    async def get_stats(self) -> CacheStats:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        async with self._lock:
            return CacheStats(
                hits=self._stats.hits,
                misses=self._stats.misses,
                evictions=self._stats.evictions,
                total_size=self._stats.total_size,
                item_count=self._stats.item_count,
            )

    def _estimate_size(self, value: Any) -> int:
        """ä¼°ç®—æ•°æ®å¤§å°ï¼ˆå­—èŠ‚ï¼‰- ä½¿ç”¨å‡†ç¡®çš„ä¼°ç®—æ–¹æ³•

        ä½¿ç”¨æ·±åº¦é€’å½’ä¼°ç®—ï¼Œæ¯” sys.getsizeof() æ›´å‡†ç¡®
        """
        try:
            return estimate_size_smart(value)
        except (TypeError, AttributeError):
            # æ— æ³•è·å–å¤§å°ï¼Œè¿”å›é»˜è®¤å€¼
            return 1024


class MultiLevelCache:
    """å¤šçº§ç¼“å­˜ç®¡ç†å™¨

    å®ç°ä¸¤çº§ç¼“å­˜æ¶æ„ï¼š
    - L1: é«˜é€Ÿç¼“å­˜ï¼Œå°å®¹é‡ï¼ŒçŸ­TTL
    - L2: æ‰©å±•ç¼“å­˜ï¼Œå¤§å®¹é‡ï¼Œé•¿TTL

    æŸ¥è¯¢æ—¶å…ˆæŸ¥L1ï¼Œæœªå‘½ä¸­å†æŸ¥L2ï¼Œæœªå‘½ä¸­å†ä»æ•°æ®æºåŠ è½½
    """

    def __init__(
        self,
        l1_max_size: int = 1000,
        l1_ttl: float = 60,
        l2_max_size: int = 10000,
        l2_ttl: float = 300,
        max_memory_mb: int = 100,
        max_item_size_mb: int = 1,
    ):
        """åˆå§‹åŒ–å¤šçº§ç¼“å­˜

        Args:
            l1_max_size: L1ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
            l1_ttl: L1ç¼“å­˜TTLï¼ˆç§’ï¼‰
            l2_max_size: L2ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
            l2_ttl: L2ç¼“å­˜TTLï¼ˆç§’ï¼‰
            max_memory_mb: æœ€å¤§å†…å­˜å ç”¨ï¼ˆMBï¼‰
            max_item_size_mb: å•ä¸ªç¼“å­˜æ¡ç›®æœ€å¤§å¤§å°ï¼ˆMBï¼‰
        """
        self.l1_cache: LRUCache[Any] = LRUCache(l1_max_size, l1_ttl, "L1")
        self.l2_cache: LRUCache[Any] = LRUCache(l2_max_size, l2_ttl, "L2")
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.max_item_size_bytes = max_item_size_mb * 1024 * 1024
        self._cleanup_task: asyncio.Task | None = None
        self._is_closing = False  # ğŸ”§ æ·»åŠ å…³é—­æ ‡å¿—

        logger.info(
            f"å¤šçº§ç¼“å­˜åˆå§‹åŒ–: L1({l1_max_size}é¡¹/{l1_ttl}s) "
            f"L2({l2_max_size}é¡¹/{l2_ttl}s) å†…å­˜ä¸Šé™({max_memory_mb}MB) "
            f"å•é¡¹ä¸Šé™({max_item_size_mb}MB)"
        )

    async def get(
        self,
        key: str,
        loader: Callable[[], Any] | None = None,
    ) -> Any | None:
        """ä»ç¼“å­˜è·å–æ•°æ®

        æŸ¥è¯¢é¡ºåºï¼šL1 -> L2 -> loader

        Args:
            key: ç¼“å­˜é”®
            loader: æ•°æ®åŠ è½½å‡½æ•°ï¼Œå½“ç¼“å­˜æœªå‘½ä¸­æ—¶è°ƒç”¨

        Returns:
            ç¼“å­˜å€¼æˆ–åŠ è½½çš„å€¼ï¼Œå¦‚æœéƒ½ä¸å­˜åœ¨è¿”å›None
        """
        # 1. å°è¯•ä»L1è·å–
        value = await self.l1_cache.get(key)
        if value is not None:
            logger.debug(f"L1ç¼“å­˜å‘½ä¸­: {key}")
            return value

        # 2. å°è¯•ä»L2è·å–
        value = await self.l2_cache.get(key)
        if value is not None:
            logger.debug(f"L2ç¼“å­˜å‘½ä¸­: {key}")
            # æå‡åˆ°L1
            await self.l1_cache.set(key, value)
            return value

        # 3. ä½¿ç”¨loaderåŠ è½½
        if loader is not None:
            logger.debug(f"ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®æºåŠ è½½: {key}")
            value = await loader() if asyncio.iscoroutinefunction(loader) else loader()
            if value is not None:
                # åŒæ—¶å†™å…¥L1å’ŒL2
                await self.set(key, value)
            return value

        return None

    async def set(
        self,
        key: str,
        value: Any,
        size: int | None = None,
        ttl: float | None = None,
    ) -> None:
        """è®¾ç½®ç¼“å­˜å€¼

        åŒæ—¶å†™å…¥L1å’ŒL2

        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
            size: æ•°æ®å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            ttl: è‡ªå®šä¹‰è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤TTL
        """
        # ä¼°ç®—æ•°æ®å¤§å°ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if size is None:
            size = estimate_size_smart(value)

        # æ£€æŸ¥å•ä¸ªæ¡ç›®å¤§å°æ˜¯å¦è¶…è¿‡é™åˆ¶
        if size > self.max_item_size_bytes:
            logger.warning(
                f"ç¼“å­˜æ¡ç›®è¿‡å¤§ï¼Œè·³è¿‡ç¼“å­˜: key={key}, "
                f"size={size / (1024 * 1024):.2f}MB, "
                f"limit={self.max_item_size_bytes / (1024 * 1024):.2f}MB"
            )
            return

        # æ ¹æ®TTLå†³å®šå†™å…¥å“ªä¸ªç¼“å­˜å±‚
        if ttl is not None:
            # æœ‰è‡ªå®šä¹‰TTLï¼Œæ ¹æ®TTLå¤§å°å†³å®šå†™å…¥å±‚çº§
            if ttl <= self.l1_cache.ttl:
                # çŸ­TTLï¼Œåªå†™å…¥L1
                await self.l1_cache.set(key, value, size, ttl)
            elif ttl <= self.l2_cache.ttl:
                # ä¸­ç­‰TTLï¼Œå†™å…¥L1å’ŒL2
                await self.l1_cache.set(key, value, size, ttl)
                await self.l2_cache.set(key, value, size, ttl)
            else:
                # é•¿TTLï¼Œåªå†™å…¥L2
                await self.l2_cache.set(key, value, size, ttl)
        else:
            # æ²¡æœ‰è‡ªå®šä¹‰TTLï¼Œä½¿ç”¨é»˜è®¤è¡Œä¸ºï¼ˆåŒæ—¶å†™å…¥L1å’ŒL2ï¼‰
            await self.l1_cache.set(key, value, size)
            await self.l2_cache.set(key, value, size)

    async def delete(self, key: str) -> None:
        """åˆ é™¤ç¼“å­˜æ¡ç›®

        åŒæ—¶ä»L1å’ŒL2åˆ é™¤

        Args:
            key: ç¼“å­˜é”®
        """
        await self.l1_cache.delete(key)
        await self.l2_cache.delete(key)

    async def clear(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        await self.l1_cache.clear()
        await self.l2_cache.clear()
        logger.info("æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º")

    async def get_stats(self) -> dict[str, Any]:
        """è·å–æ‰€æœ‰ç¼“å­˜å±‚çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿®æ­£ç‰ˆï¼Œé¿å…é‡å¤è®¡æ•°ï¼‰"""
        l1_stats = await self.l1_cache.get_stats()
        l2_stats = await self.l2_cache.get_stats()

        # ğŸ”§ ä¿®å¤ï¼šè®¡ç®—å®é™…ç‹¬å çš„å†…å­˜ï¼Œé¿å…L1å’ŒL2å…±äº«æ•°æ®çš„é‡å¤è®¡æ•°
        l1_keys = set(self.l1_cache._cache.keys())
        l2_keys = set(self.l2_cache._cache.keys())

        shared_keys = l1_keys & l2_keys
        l1_only_keys = l1_keys - l2_keys
        l2_only_keys = l2_keys - l1_keys

        # è®¡ç®—å®é™…æ€»å†…å­˜ï¼ˆé¿å…é‡å¤è®¡æ•°ï¼‰
        # L1ç‹¬å å†…å­˜
        l1_only_size = sum(
            self.l1_cache._cache[k].size
            for k in l1_only_keys
            if k in self.l1_cache._cache
        )
        # L2ç‹¬å å†…å­˜
        l2_only_size = sum(
            self.l2_cache._cache[k].size
            for k in l2_only_keys
            if k in self.l2_cache._cache
        )
        # å…±äº«å†…å­˜ï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼Œä½¿ç”¨L1çš„æ•°æ®ï¼‰
        shared_size = sum(
            self.l1_cache._cache[k].size
            for k in shared_keys
            if k in self.l1_cache._cache
        )

        actual_total_size = l1_only_size + l2_only_size + shared_size

        return {
            "l1": l1_stats,
            "l2": l2_stats,
            "total_memory_mb": actual_total_size / (1024 * 1024),
            "l1_only_mb": l1_only_size / (1024 * 1024),
            "l2_only_mb": l2_only_size / (1024 * 1024),
            "shared_mb": shared_size / (1024 * 1024),
            "shared_keys_count": len(shared_keys),
            "dedup_savings_mb": (l1_stats.total_size + l2_stats.total_size - actual_total_size) / (1024 * 1024),
            "max_memory_mb": self.max_memory_bytes / (1024 * 1024),
            "memory_usage_percent": (actual_total_size / self.max_memory_bytes * 100) if self.max_memory_bytes > 0 else 0,
        }

    async def check_memory_limit(self) -> None:
        """æ£€æŸ¥å¹¶å¼ºåˆ¶æ¸…ç†è¶…å‡ºå†…å­˜é™åˆ¶çš„ç¼“å­˜"""
        stats = await self.get_stats()
        total_size = stats["l1"].total_size + stats["l2"].total_size

        if total_size > self.max_memory_bytes:
            memory_mb = total_size / (1024 * 1024)
            max_mb = self.max_memory_bytes / (1024 * 1024)
            logger.warning(
                f"ç¼“å­˜å†…å­˜è¶…é™: {memory_mb:.2f}MB / {max_mb:.2f}MB "
                f"({stats['memory_usage_percent']:.1f}%)ï¼Œå¼€å§‹å¼ºåˆ¶æ¸…ç†L2ç¼“å­˜"
            )
            # ä¼˜å…ˆæ¸…ç†L2ç¼“å­˜ï¼ˆæ¸©æ•°æ®ï¼‰
            await self.l2_cache.clear()

            # å¦‚æœæ¸…ç†L2åä»è¶…é™ï¼Œæ¸…ç†L1
            stats_after_l2 = await self.get_stats()
            total_after_l2 = stats_after_l2["l1"].total_size + stats_after_l2["l2"].total_size
            if total_after_l2 > self.max_memory_bytes:
                logger.warning("æ¸…ç†L2åä»è¶…é™ï¼Œç»§ç»­æ¸…ç†L1ç¼“å­˜")
                await self.l1_cache.clear()

            logger.info("ç¼“å­˜å¼ºåˆ¶æ¸…ç†å®Œæˆ")

    async def start_cleanup_task(self, interval: float = 60) -> None:
        """å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡

        Args:
            interval: æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
        """
        if self._cleanup_task is not None:
            logger.warning("æ¸…ç†ä»»åŠ¡å·²åœ¨è¿è¡Œ")
            return

        async def cleanup_loop():
            while not self._is_closing:
                try:
                    await asyncio.sleep(interval)

                    if self._is_closing:
                        break

                    stats = await self.get_stats()
                    l1_stats = stats["l1"]
                    l2_stats = stats["l2"]
                    logger.info(
                        f"ç¼“å­˜ç»Ÿè®¡ - L1: {l1_stats.item_count}é¡¹, "
                        f"å‘½ä¸­ç‡{l1_stats.hit_rate:.2%} | "
                        f"L2: {l2_stats.item_count}é¡¹, "
                        f"å‘½ä¸­ç‡{l2_stats.hit_rate:.2%} | "
                        f"å†…å­˜: {stats['total_memory_mb']:.2f}MB/{stats['max_memory_mb']:.2f}MB "
                        f"({stats['memory_usage_percent']:.1f}%) | "
                        f"å…±äº«: {stats['shared_keys_count']}é”®/{stats['shared_mb']:.2f}MB "
                        f"(å»é‡èŠ‚çœ{stats['dedup_savings_mb']:.2f}MB)"
                    )

                    # ğŸ”§ æ¸…ç†è¿‡æœŸæ¡ç›®
                    await self._clean_expired_entries()

                    # æ£€æŸ¥å†…å­˜é™åˆ¶
                    await self.check_memory_limit()

                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}", exc_info=True)

        self._cleanup_task = asyncio.create_task(cleanup_loop())
        logger.info(f"ç¼“å­˜æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨ï¼Œé—´éš”{interval}ç§’")

    async def stop_cleanup_task(self) -> None:
        """åœæ­¢æ¸…ç†ä»»åŠ¡"""
        self._is_closing = True

        if self._cleanup_task is not None:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
            self._cleanup_task = None
            logger.info("ç¼“å­˜æ¸…ç†ä»»åŠ¡å·²åœæ­¢")

    async def _clean_expired_entries(self) -> None:
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ¡ç›®"""
        try:
            current_time = time.time()

            # æ¸…ç† L1 è¿‡æœŸæ¡ç›®
            async with self.l1_cache._lock:
                expired_keys = [
                    key for key, entry in self.l1_cache._cache.items()
                    if current_time - entry.created_at > self.l1_cache.ttl
                ]

                for key in expired_keys:
                    entry = self.l1_cache._cache.pop(key, None)
                    if entry:
                        self.l1_cache._stats.evictions += 1
                        self.l1_cache._stats.item_count -= 1
                        self.l1_cache._stats.total_size -= entry.size

            # æ¸…ç† L2 è¿‡æœŸæ¡ç›®
            async with self.l2_cache._lock:
                expired_keys = [
                    key for key, entry in self.l2_cache._cache.items()
                    if current_time - entry.created_at > self.l2_cache.ttl
                ]

                for key in expired_keys:
                    entry = self.l2_cache._cache.pop(key, None)
                    if entry:
                        self.l2_cache._stats.evictions += 1
                        self.l2_cache._stats.item_count -= 1
                        self.l2_cache._stats.total_size -= entry.size

            if expired_keys:
                logger.debug(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜æ¡ç›®")

        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸæ¡ç›®å¤±è´¥: {e}", exc_info=True)


# å…¨å±€ç¼“å­˜å®ä¾‹
_global_cache: MultiLevelCache | None = None
_cache_lock = asyncio.Lock()


async def get_cache() -> MultiLevelCache:
    """è·å–å…¨å±€ç¼“å­˜å®ä¾‹ï¼ˆå•ä¾‹ï¼‰

    ä»é…ç½®æ–‡ä»¶è¯»å–ç¼“å­˜å‚æ•°ï¼Œå¦‚æœé…ç½®æœªåŠ è½½åˆ™ä½¿ç”¨é»˜è®¤å€¼
    å¦‚æœé…ç½®ä¸­ç¦ç”¨äº†ç¼“å­˜ï¼Œè¿”å›ä¸€ä¸ªæœ€å°åŒ–çš„ç¼“å­˜å®ä¾‹ï¼ˆå®¹é‡ä¸º1ï¼‰
    """
    global _global_cache

    if _global_cache is None:
        async with _cache_lock:
            if _global_cache is None:
                # å°è¯•ä»é…ç½®è¯»å–å‚æ•°
                try:
                    from src.config.config import global_config

                    db_config = global_config.database

                    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç¼“å­˜
                    if not db_config.enable_database_cache:
                        logger.info("æ•°æ®åº“ç¼“å­˜å·²ç¦ç”¨ï¼Œä½¿ç”¨æœ€å°åŒ–ç¼“å­˜å®ä¾‹")
                        _global_cache = MultiLevelCache(
                            l1_max_size=1,
                            l1_ttl=1,
                            l2_max_size=1,
                            l2_ttl=1,
                            max_memory_mb=1,
                        )
                        return _global_cache

                    l1_max_size = db_config.cache_l1_max_size
                    l1_ttl = db_config.cache_l1_ttl
                    l2_max_size = db_config.cache_l2_max_size
                    l2_ttl = db_config.cache_l2_ttl
                    max_memory_mb = db_config.cache_max_memory_mb
                    max_item_size_mb = db_config.cache_max_item_size_mb
                    cleanup_interval = db_config.cache_cleanup_interval

                    logger.info(
                        f"ä»é…ç½®åŠ è½½ç¼“å­˜å‚æ•°: L1({l1_max_size}/{l1_ttl}s), "
                        f"L2({l2_max_size}/{l2_ttl}s), å†…å­˜é™åˆ¶({max_memory_mb}MB), "
                        f"å•é¡¹é™åˆ¶({max_item_size_mb}MB)"
                    )
                except Exception as e:
                    # é…ç½®æœªåŠ è½½ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    logger.warning(f"æ— æ³•ä»é…ç½®åŠ è½½ç¼“å­˜å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                    l1_max_size = 1000
                    l1_ttl = 60
                    l2_max_size = 10000
                    l2_ttl = 300
                    max_memory_mb = 100
                    max_item_size_mb = 1
                    cleanup_interval = 60

                _global_cache = MultiLevelCache(
                    l1_max_size=l1_max_size,
                    l1_ttl=l1_ttl,
                    l2_max_size=l2_max_size,
                    l2_ttl=l2_ttl,
                    max_memory_mb=max_memory_mb,
                    max_item_size_mb=max_item_size_mb,
                )
                await _global_cache.start_cleanup_task(interval=cleanup_interval)

    return _global_cache


async def close_cache() -> None:
    """å…³é—­å…¨å±€ç¼“å­˜"""
    global _global_cache

    if _global_cache is not None:
        await _global_cache.stop_cleanup_task()
        await _global_cache.clear()
        _global_cache = None
        logger.info("å…¨å±€ç¼“å­˜å·²å…³é—­")
