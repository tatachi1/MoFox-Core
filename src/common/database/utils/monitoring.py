"""æ•°æ®åº“æ€§èƒ½ç›‘æ§

æä¾›æ•°æ®åº“æ“ä½œçš„æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡åŠŸèƒ½
"""

import time
from collections import deque
from dataclasses import dataclass, field
from typing import Any, Optional

from src.common.logger import get_logger

logger = get_logger("database.monitoring")


@dataclass
class SlowQueryRecord:
    """æ…¢æŸ¥è¯¢è®°å½•"""

    operation_name: str
    execution_time: float
    timestamp: float
    sql: str | None = None
    args: tuple | None = None
    stack_trace: str | None = None

    def __str__(self) -> str:
        return (
            f"[{self.operation_name}] {self.execution_time:.3f}s "
            f"@ {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(self.timestamp))}"
        )


@dataclass
class OperationMetrics:
    """æ“ä½œæŒ‡æ ‡"""

    count: int = 0
    total_time: float = 0.0
    min_time: float = float("inf")
    max_time: float = 0.0
    error_count: int = 0
    last_execution_time: float | None = None
    slow_query_count: int = 0  # è¯¥æ“ä½œçš„æ…¢æŸ¥è¯¢æ•°

    @property
    def avg_time(self) -> float:
        """å¹³å‡æ‰§è¡Œæ—¶é—´"""
        return self.total_time / self.count if self.count > 0 else 0.0

    def record_success(self, execution_time: float):
        """è®°å½•æˆåŠŸæ‰§è¡Œ"""
        self.count += 1
        self.total_time += execution_time
        self.min_time = min(self.min_time, execution_time)
        self.max_time = max(self.max_time, execution_time)
        self.last_execution_time = time.time()

    def record_error(self):
        """è®°å½•é”™è¯¯"""
        self.error_count += 1

    def record_slow_query(self):
        """è®°å½•æ…¢æŸ¥è¯¢"""
        self.slow_query_count += 1


@dataclass
class DatabaseMetrics:
    """æ•°æ®åº“æŒ‡æ ‡"""

    # æ“ä½œç»Ÿè®¡
    operations: dict[str, OperationMetrics] = field(default_factory=dict)

    # è¿æ¥æ± ç»Ÿè®¡
    connection_acquired: int = 0
    connection_released: int = 0
    connection_errors: int = 0

    # ç¼“å­˜ç»Ÿè®¡
    cache_hits: int = 0
    cache_misses: int = 0
    cache_sets: int = 0
    cache_invalidations: int = 0

    # æ‰¹å¤„ç†ç»Ÿè®¡
    batch_operations: int = 0
    batch_items_total: int = 0
    batch_avg_size: float = 0.0

    # æ…¢æŸ¥è¯¢ç»Ÿè®¡
    slow_query_count: int = 0
    slow_query_threshold: float = 0.5  # æ…¢æŸ¥è¯¢é˜ˆå€¼

    @property
    def cache_hit_rate(self) -> float:
        """ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0

    @property
    def error_rate(self) -> float:
        """é”™è¯¯ç‡"""
        total_ops = sum(m.count for m in self.operations.values())
        total_errors = sum(m.error_count for m in self.operations.values())
        return total_errors / total_ops if total_ops > 0 else 0.0

    def get_operation_metrics(self, operation_name: str) -> OperationMetrics:
        """è·å–æ“ä½œæŒ‡æ ‡"""
        if operation_name not in self.operations:
            self.operations[operation_name] = OperationMetrics()
        return self.operations[operation_name]


class DatabaseMonitor:
    """æ•°æ®åº“ç›‘æ§å™¨

    å•ä¾‹æ¨¡å¼ï¼Œæ”¶é›†å’ŒæŠ¥å‘Šæ•°æ®åº“æ€§èƒ½æŒ‡æ ‡
    """

    _instance: Optional["DatabaseMonitor"] = None
    _metrics: DatabaseMetrics
    _slow_queries: deque  # æœ€è¿‘çš„æ…¢æŸ¥è¯¢è®°å½•
    _slow_query_buffer_size: int = 100
    _enabled: bool = False  # æ…¢æŸ¥è¯¢ç›‘æ§æ˜¯å¦å¯ç”¨

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._metrics = DatabaseMetrics()
            cls._instance._slow_queries = deque(maxlen=cls._slow_query_buffer_size)
            cls._instance._enabled = False
        return cls._instance

    def enable(self):
        """å¯ç”¨æ…¢æŸ¥è¯¢ç›‘æ§"""
        self._enabled = True
        logger.info("âœ… æ…¢æŸ¥è¯¢ç›‘æ§å·²å¯ç”¨")

    def disable(self):
        """ç¦ç”¨æ…¢æŸ¥è¯¢ç›‘æ§"""
        self._enabled = False
        logger.info("âŒ æ…¢æŸ¥è¯¢ç›‘æ§å·²ç¦ç”¨")

    def is_enabled(self) -> bool:
        """æ£€æŸ¥æ…¢æŸ¥è¯¢ç›‘æ§æ˜¯å¦å¯ç”¨"""
        return self._enabled

    def set_slow_query_config(self, threshold: float, buffer_size: int):
        """è®¾ç½®æ…¢æŸ¥è¯¢é…ç½®"""
        self._metrics.slow_query_threshold = threshold
        self._slow_query_buffer_size = buffer_size
        self._slow_queries = deque(maxlen=buffer_size)
        # è®¾ç½®é…ç½®æ—¶è‡ªåŠ¨å¯ç”¨
        self._enabled = True

    def record_operation(
        self,
        operation_name: str,
        execution_time: float,
        success: bool = True,
        sql: str | None = None,
    ):
        """è®°å½•æ“ä½œ"""
        metrics = self._metrics.get_operation_metrics(operation_name)
        if success:
            metrics.record_success(execution_time)

            # åªåœ¨å¯ç”¨æ—¶æ£€æŸ¥æ˜¯å¦ä¸ºæ…¢æŸ¥è¯¢
            if self._enabled and execution_time > self._metrics.slow_query_threshold:
                self.record_slow_query(operation_name, execution_time, sql)
        else:
            metrics.record_error()

    def record_slow_query(
        self,
        operation_name: str,
        execution_time: float,
        sql: str | None = None,
        args: tuple | None = None,
        stack_trace: str | None = None,
    ):
        """è®°å½•æ…¢æŸ¥è¯¢"""
        self._metrics.slow_query_count += 1
        self._metrics.get_operation_metrics(operation_name).record_slow_query()

        record = SlowQueryRecord(
            operation_name=operation_name,
            execution_time=execution_time,
            timestamp=time.time(),
            sql=sql,
            args=args,
            stack_trace=stack_trace,
        )
        self._slow_queries.append(record)

        # ç«‹å³è®°å½•åˆ°æ—¥å¿—ï¼ˆå®æ—¶å‘Šè­¦ï¼‰
        logger.warning(f"ğŸ¢ æ…¢æŸ¥è¯¢: {record}")

    def record_connection_acquired(self):
        """è®°å½•è¿æ¥è·å–"""
        self._metrics.connection_acquired += 1

    def record_connection_released(self):
        """è®°å½•è¿æ¥é‡Šæ”¾"""
        self._metrics.connection_released += 1

    def record_connection_error(self):
        """è®°å½•è¿æ¥é”™è¯¯"""
        self._metrics.connection_errors += 1

    def record_cache_hit(self):
        """è®°å½•ç¼“å­˜å‘½ä¸­"""
        self._metrics.cache_hits += 1

    def record_cache_miss(self):
        """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
        self._metrics.cache_misses += 1

    def record_cache_set(self):
        """è®°å½•ç¼“å­˜è®¾ç½®"""
        self._metrics.cache_sets += 1

    def record_cache_invalidation(self):
        """è®°å½•ç¼“å­˜å¤±æ•ˆ"""
        self._metrics.cache_invalidations += 1

    def record_batch_operation(self, batch_size: int):
        """è®°å½•æ‰¹å¤„ç†æ“ä½œ"""
        self._metrics.batch_operations += 1
        self._metrics.batch_items_total += batch_size
        self._metrics.batch_avg_size = (
            self._metrics.batch_items_total / self._metrics.batch_operations
        )

    def get_metrics(self) -> DatabaseMetrics:
        """è·å–æŒ‡æ ‡"""
        return self._metrics

    def get_slow_queries(self, limit: int = 0) -> list[SlowQueryRecord]:
        """è·å–æ…¢æŸ¥è¯¢è®°å½•

        Args:
            limit: è¿”å›æ•°é‡é™åˆ¶ï¼Œ0 è¡¨ç¤ºè¿”å›å…¨éƒ¨

        Returns:
            æ…¢æŸ¥è¯¢è®°å½•åˆ—è¡¨
        """
        records = list(self._slow_queries)
        if limit > 0:
            records = records[-limit:]
        return records

    def get_slow_query_report(self) -> dict[str, Any]:
        """è·å–æ…¢æŸ¥è¯¢æŠ¥å‘Š"""
        slow_queries = list(self._slow_queries)

        if not slow_queries:
            return {
                "total": 0,
                "threshold": f"{self._metrics.slow_query_threshold:.3f}s",
                "top_operations": [],
                "recent_queries": [],
            }

        # æŒ‰æ“ä½œåˆ†ç»„ç»Ÿè®¡
        operation_stats = {}
        for record in slow_queries:
            if record.operation_name not in operation_stats:
                operation_stats[record.operation_name] = {
                    "count": 0,
                    "total_time": 0.0,
                    "max_time": 0.0,
                    "min_time": float("inf"),
                }
            stats = operation_stats[record.operation_name]
            stats["count"] += 1
            stats["total_time"] += record.execution_time
            stats["max_time"] = max(stats["max_time"], record.execution_time)
            stats["min_time"] = min(stats["min_time"], record.execution_time)

        # æŒ‰æ…¢æŸ¥è¯¢æ•°æ’åº
        top_operations = sorted(
            operation_stats.items(),
            key=lambda x: x[1]["count"],
            reverse=True,
        )[:10]

        return {
            "total": len(slow_queries),
            "threshold": f"{self._metrics.slow_query_threshold:.3f}s",
            "top_operations": [
                {
                    "operation": op_name,
                    "count": stats["count"],
                    "avg_time": f"{stats['total_time'] / stats['count']:.3f}s",
                    "max_time": f"{stats['max_time']:.3f}s",
                    "min_time": f"{stats['min_time']:.3f}s",
                }
                for op_name, stats in top_operations
            ],
            "recent_queries": [
                {
                    "operation": record.operation_name,
                    "time": f"{record.execution_time:.3f}s",
                    "timestamp": time.strftime(
                        "%Y-%m-%d %H:%M:%S",
                        time.localtime(record.timestamp),
                    ),
                }
                for record in slow_queries[-20:]
            ],
        }

    def get_summary(self) -> dict[str, Any]:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        metrics = self._metrics

        operation_summary = {}
        for op_name, op_metrics in metrics.operations.items():
            operation_summary[op_name] = {
                "count": op_metrics.count,
                "avg_time": f"{op_metrics.avg_time:.3f}s",
                "min_time": f"{op_metrics.min_time:.3f}s",
                "max_time": f"{op_metrics.max_time:.3f}s",
                "error_count": op_metrics.error_count,
                "slow_query_count": op_metrics.slow_query_count,
            }

        return {
            "operations": operation_summary,
            "connections": {
                "acquired": metrics.connection_acquired,
                "released": metrics.connection_released,
                "errors": metrics.connection_errors,
                "active": metrics.connection_acquired - metrics.connection_released,
            },
            "cache": {
                "hits": metrics.cache_hits,
                "misses": metrics.cache_misses,
                "sets": metrics.cache_sets,
                "invalidations": metrics.cache_invalidations,
                "hit_rate": f"{metrics.cache_hit_rate:.2%}",
            },
            "batch": {
                "operations": metrics.batch_operations,
                "total_items": metrics.batch_items_total,
                "avg_size": f"{metrics.batch_avg_size:.1f}",
            },
            "overall": {
                "error_rate": f"{metrics.error_rate:.2%}",
                "slow_query_count": metrics.slow_query_count,
                "slow_query_threshold": f"{metrics.slow_query_threshold:.3f}s",
            },
        }

    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        summary = self.get_summary()

        logger.info("=" * 60)
        logger.info("æ•°æ®åº“æ€§èƒ½ç»Ÿè®¡")
        logger.info("=" * 60)

        # æ“ä½œç»Ÿè®¡
        if summary["operations"]:
            logger.info("\næ“ä½œç»Ÿè®¡:")
            for op_name, stats in summary["operations"].items():
                logger.info(
                    f"  {op_name}: "
                    f"æ¬¡æ•°={stats['count']}, "
                    f"å¹³å‡={stats['avg_time']}, "
                    f"æœ€å°={stats['min_time']}, "
                    f"æœ€å¤§={stats['max_time']}, "
                    f"é”™è¯¯={stats['error_count']}, "
                    f"æ…¢æŸ¥è¯¢={stats['slow_query_count']}"
                )

        # è¿æ¥æ± ç»Ÿè®¡
        logger.info("\nè¿æ¥æ± :")
        conn = summary["connections"]
        logger.info(
            f"  è·å–={conn['acquired']}, "
            f"é‡Šæ”¾={conn['released']}, "
            f"æ´»è·ƒ={conn['active']}, "
            f"é”™è¯¯={conn['errors']}"
        )

        # ç¼“å­˜ç»Ÿè®¡
        logger.info("\nç¼“å­˜:")
        cache = summary["cache"]
        logger.info(
            f"  å‘½ä¸­={cache['hits']}, "
            f"æœªå‘½ä¸­={cache['misses']}, "
            f"è®¾ç½®={cache['sets']}, "
            f"å¤±æ•ˆ={cache['invalidations']}, "
            f"å‘½ä¸­ç‡={cache['hit_rate']}"
        )

        # æ‰¹å¤„ç†ç»Ÿè®¡
        logger.info("\næ‰¹å¤„ç†:")
        batch = summary["batch"]
        logger.info(
            f"  æ“ä½œ={batch['operations']}, "
            f"æ€»é¡¹ç›®={batch['total_items']}, "
            f"å¹³å‡å¤§å°={batch['avg_size']}"
        )

        # æ•´ä½“ç»Ÿè®¡
        logger.info("\næ•´ä½“:")
        overall = summary["overall"]
        logger.info(f"  é”™è¯¯ç‡={overall['error_rate']}")
        logger.info(f"  æ…¢æŸ¥è¯¢æ€»æ•°={overall['slow_query_count']}")
        logger.info(f"  æ…¢æŸ¥è¯¢é˜ˆå€¼={overall['slow_query_threshold']}")

        # æ…¢æŸ¥è¯¢æŠ¥å‘Š
        if overall["slow_query_count"] > 0:
            logger.info("\nğŸ¢ æ…¢æŸ¥è¯¢æŠ¥å‘Š:")
            slow_report = self.get_slow_query_report()

            if slow_report["top_operations"]:
                logger.info("  æŒ‰æ“ä½œæ’åï¼ˆTop 10ï¼‰:")
                for idx, op in enumerate(slow_report["top_operations"], 1):
                    logger.info(
                        f"    {idx}. {op['operation']}: "
                        f"æ¬¡æ•°={op['count']}, "
                        f"å¹³å‡={op['avg_time']}, "
                        f"æœ€å¤§={op['max_time']}"
                    )


        logger.info("=" * 60)

    def reset(self):
        """é‡ç½®ç»Ÿè®¡"""
        self._metrics = DatabaseMetrics()
        logger.info("æ•°æ®åº“ç›‘æ§ç»Ÿè®¡å·²é‡ç½®")


# å…¨å±€ç›‘æ§å™¨å®ä¾‹
_monitor: DatabaseMonitor | None = None


def get_monitor() -> DatabaseMonitor:
    """è·å–ç›‘æ§å™¨å®ä¾‹"""
    global _monitor
    if _monitor is None:
        _monitor = DatabaseMonitor()
    return _monitor


# ä¾¿æ·å‡½æ•°
def record_operation(operation_name: str, execution_time: float, success: bool = True):
    """è®°å½•æ“ä½œ"""
    get_monitor().record_operation(operation_name, execution_time, success)


def record_slow_query(
    operation_name: str,
    execution_time: float,
    sql: str | None = None,
    args: tuple | None = None,
):
    """è®°å½•æ…¢æŸ¥è¯¢"""
    get_monitor().record_slow_query(operation_name, execution_time, sql, args)


def get_slow_queries(limit: int = 0) -> list[SlowQueryRecord]:
    """è·å–æ…¢æŸ¥è¯¢è®°å½•"""
    return get_monitor().get_slow_queries(limit)


def get_slow_query_report() -> dict[str, Any]:
    """è·å–æ…¢æŸ¥è¯¢æŠ¥å‘Š"""
    return get_monitor().get_slow_query_report()


def set_slow_query_config(threshold: float, buffer_size: int):
    """è®¾ç½®æ…¢æŸ¥è¯¢é…ç½®"""
    get_monitor().set_slow_query_config(threshold, buffer_size)


def enable_slow_query_monitoring():
    """å¯ç”¨æ…¢æŸ¥è¯¢ç›‘æ§"""
    get_monitor().enable()


def disable_slow_query_monitoring():
    """ç¦ç”¨æ…¢æŸ¥è¯¢ç›‘æ§"""
    get_monitor().disable()


def is_slow_query_monitoring_enabled() -> bool:
    """æ£€æŸ¥æ…¢æŸ¥è¯¢ç›‘æ§æ˜¯å¦å¯ç”¨"""
    return get_monitor().is_enabled()


def record_cache_hit():
    """è®°å½•ç¼“å­˜å‘½ä¸­"""
    get_monitor().record_cache_hit()


def record_cache_miss():
    """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
    get_monitor().record_cache_miss()


def print_stats():
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    get_monitor().print_summary()


def reset_stats():
    """é‡ç½®ç»Ÿè®¡"""
    get_monitor().reset()
